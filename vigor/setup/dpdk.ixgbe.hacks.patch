--- a/drivers/net/ixgbe/ixgbe_rxtx.c	2020-09-20 12:08:34.572132233 +0000
+++ b/drivers/net/ixgbe/ixgbe_rxtx.c	2020-09-20 12:05:50.143740047 +0000
@@ -1572,9 +1572,11 @@
 	}
 
 	/* clear software ring entries so we can cleanup correctly */
+#ifndef KLEE_VERIFICATION
 	for (i = 0; i < nb_rx; ++i) {
 		rxq->sw_ring[rxq->rx_tail + i].mbuf = NULL;
 	}
+#endif
 
 
 	return nb_rx;
@@ -1662,7 +1664,9 @@
 	/* update internal queue state */
 	rxq->rx_next_avail = 0;
 	rxq->rx_nb_avail = nb_rx;
+#ifndef KLEE_VERIFICATION
 	rxq->rx_tail = (uint16_t)(rxq->rx_tail + nb_rx);
+#endif
 
 	/* if required, allocate new buffers to replenish descriptors */
 	if (rxq->rx_tail > rxq->rx_free_trigger) {
@@ -1696,8 +1700,10 @@
 					    cur_free_trigger);
 	}
 
+#ifndef KLEE_VERIFICATION
 	if (rxq->rx_tail >= rxq->nb_rx_desc)
 		rxq->rx_tail = 0;
+#endif
 
 	/* received any packets this loop? */
 	if (rxq->rx_nb_avail)
@@ -1840,7 +1846,9 @@
 		}
 
 		rxm = rxe->mbuf;
+#ifndef KLEE_VERIFICATION
 		rxe->mbuf = nmb;
+#endif
 		dma_addr =
 			rte_cpu_to_le_64(rte_mbuf_data_iova_default(nmb));
 		rxdp->read.hdr_addr = 0;
@@ -1898,7 +1906,9 @@
 		 */
 		rx_pkts[nb_rx++] = rxm;
 	}
+#ifndef KLEE_VERIFICATION
 	rxq->rx_tail = rx_id;
+#endif
 
 	/*
 	 * If the number of free RX descriptors is greater than the RX free
@@ -2140,13 +2150,18 @@
 			 * Update RX descriptor with the physical address of the
 			 * new data buffer of the new allocated mbuf.
 			 */
+#ifndef KLEE_VERIFICATION
 			rxe->mbuf = nmb;
+#endif
 
 			rxm->data_off = RTE_PKTMBUF_HEADROOM;
 			rxdp->read.hdr_addr = 0;
 			rxdp->read.pkt_addr = dma;
-		} else
+		} else {
+#ifndef KLEE_VERIFICATION
 			rxe->mbuf = NULL;
+#endif
+		}
 
 		/*
 		 * Set data length & data buffer address of mbuf.
@@ -2242,10 +2257,12 @@
 		rx_pkts[nb_rx++] = first_seg;
 	}
 
+#ifndef KLEE_VERIFICATION
 	/*
 	 * Record index of the next RX descriptor to probe.
 	 */
 	rxq->rx_tail = rx_id;
+#endif
 
 	/*
 	 * If the number of free RX descriptors is greater than the RX free
@@ -2804,7 +2821,9 @@
 		for (i = 0; i < rxq->nb_rx_desc; i++) {
 			if (rxq->sw_ring[i].mbuf != NULL) {
 				rte_pktmbuf_free_seg(rxq->sw_ring[i].mbuf);
+#ifndef KLEE_VERIFICATION
 				rxq->sw_ring[i].mbuf = NULL;
+#endif
 			}
 		}
 		if (rxq->rx_nb_avail) {
--- a/drivers/net/ixgbe/ixgbe_rxtx_vec_sse.c	2020-05-26 15:41:39.000000000 +0000
+++ b/drivers/net/ixgbe/ixgbe_rxtx_vec_sse.c	2020-09-20 12:05:50.075743194 +0000
@@ -650,8 +650,10 @@
 	/* cross rx_thresh boundary is not allowed */
 	nb_pkts = RTE_MIN(nb_pkts, txq->tx_rs_thresh);
 
+#ifndef KLEE_VERIFICATION
 	if (txq->nb_tx_free < txq->tx_free_thresh)
 		ixgbe_tx_free_bufs(txq);
+#endif
 
 	nb_commit = nb_pkts = (uint16_t)RTE_MIN(txq->nb_tx_free, nb_pkts);
 	if (unlikely(nb_pkts == 0))
@@ -661,7 +663,9 @@
 	txdp = &txq->tx_ring[tx_id];
 	txep = &txq->sw_ring_v[tx_id];
 
+#ifndef KLEE_VERIFICATION
 	txq->nb_tx_free = (uint16_t)(txq->nb_tx_free - nb_pkts);
+#endif
 
 	n = (uint16_t)(txq->nb_tx_desc - tx_id);
 	if (nb_commit >= n) {
@@ -695,7 +699,10 @@
 			txq->tx_rs_thresh);
 	}
 
+	txq->tx_tail = 0;
+#ifndef KLEE_VERIFICATION
 	txq->tx_tail = tx_id;
+#endif
 
 	IXGBE_PCI_REG_WRITE(txq->tdt_reg_addr, txq->tx_tail);
 
--- a/lib/librte_mbuf/rte_mbuf.h	2020-05-26 15:41:39.000000000 +0000
+++ b/lib/librte_mbuf/rte_mbuf.h	2020-09-20 12:05:50.083742823 +0000
@@ -612,7 +612,9 @@
 	RTE_ASSERT(m->next == NULL);
 	RTE_ASSERT(m->nb_segs == 1);
 	__rte_mbuf_sanity_check(m, 0);
+#ifndef KLEE_VERIFICATION
 	rte_mempool_put(m->pool, m);
+#endif
 }
 
 /**
